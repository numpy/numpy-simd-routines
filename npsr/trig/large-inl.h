#if defined(NPSR_TRIG_LARGE_INL_H_) == defined(HWY_TARGET_TOGGLE)  // NOLINT
#ifdef NPSR_TRIG_LARGE_INL_H_
#undef NPSR_TRIG_LARGE_INL_H_
#else
#define NPSR_TRIG_LARGE_INL_H_
#endif

#include "npsr/common.h"
#include "npsr/trig/data/large-aprox.h"
#include "npsr/trig/data/large-reduction.h"

HWY_BEFORE_NAMESPACE();
namespace npsr::HWY_NAMESPACE::sincos {

template <bool IS_COS, class V>
HWY_API V LargeArg(V x) {
  using namespace hwy;
  using namespace hwy::HWY_NAMESPACE;
  using trig::data::kCosApproxTable;
  using trig::data::kLargeReductionTable;
  using trig::data::kSinApproxTable;

  using D = DFromV<V>;
  using DI = RebindToSigned<D>;
  using DU = RebindToUnsigned<D>;
  using VI = Vec<DI>;
  using VU = Vec<DU>;
  using T = TFromV<V>;
  using TU = TFromV<VU>;
  const D d;
  const DI di;
  const DU du;

  constexpr bool kIsSingle = std::is_same_v<T, float>;

  // =============================================================================
  // PHASE 1: Table Lookup for Reduction Constants
  // =============================================================================
  // Each table entry contains 3 consecutive values [high, mid, low]
  // providing ~96-bit (F32) or ~192-bit (F64) precision for (4/π) × 2^exp
  VU u_exponent = GetBiasedExponent(x);
  VI i_table_idx =
      BitCast(di, Add(ShiftLeft<1>(u_exponent), u_exponent));  // × 2 + 1 = × 3

  // Gather three parts of (4/π) × 2^exp from precomputed table
  // Generated by Python script with offset: 70 (F32) or 137 (F64)
  VU u_p_hi = GatherIndex(du, kLargeReductionTable<T>, i_table_idx);
  VU u_p_med = GatherIndex(du, kLargeReductionTable<T> + 1, i_table_idx);
  VU u_p_lo = GatherIndex(du, kLargeReductionTable<T> + 2, i_table_idx);

  // =============================================================================
  // PHASE 2: Extract and Normalize Mantissa
  // =============================================================================

  // Extract and Normalize Mantissa
  V abx = Abs(x);
  VU u_input = BitCast(du, abx);
  VU u_significand = And(u_input, Set(du, MantissaMask<T>()));
  // Add implicit leading 1 bit
  VU u_integer_bit =
      Or(u_significand, Set(du, static_cast<TU>(1) << MantissaBits<T>()));
  VU u_mantissa = Or(u_significand, u_integer_bit);

  // Split mantissa into halves for extended precision multiplication
  // F32: 16-bit halves, F64: 32-bit halves
  constexpr int kHalfShift = (sizeof(T) / 2) * 8;
  VU u_low_mask = Set(du, (static_cast<TU>(1) << kHalfShift) - 1);
  VU u_m0 = And(u_mantissa, u_low_mask);
  VU u_m1 = ShiftRight<kHalfShift>(u_mantissa);

  // Split reduction constants into halves
  VU u_p0 = And(u_p_lo, u_low_mask);
  VU u_p1 = ShiftRight<kHalfShift>(u_p_lo);
  VU u_p2 = And(u_p_med, u_low_mask);
  VU u_p3 = ShiftRight<kHalfShift>(u_p_med);
  VU u_p4 = And(u_p_hi, u_low_mask);
  VU u_p5 = ShiftRight<kHalfShift>(u_p_hi);

  // =============================================================================
  // PHASE 3: Extended Precision Multiplication
  // =============================================================================
  // mantissa × (4/π × 2^exp) using half-word multiplications
  // F32: 16×16→32 bit, F64: 32×32→64 bit multiplications

  // Products with highest precision part
  VU u_m04 = Mul(u_m0, u_p4);
  VU u_m05 = Mul(u_m0, u_p5);
  VU u_m14 = Mul(u_m1, u_p4);
  // Omit u_m1 × u_p5 to prevent overflow

  // Products with medium precision part
  VU u_m02 = Mul(u_m0, u_p2);
  VU u_m03 = Mul(u_m0, u_p3);
  VU u_m12 = Mul(u_m1, u_p2);
  VU u_m13 = Mul(u_m1, u_p3);

  // Products with lowest precision part
  VU u_m01 = Mul(u_m0, u_p1);
  VU u_m10 = Mul(u_m1, u_p0);
  VU u_m11 = Mul(u_m1, u_p1);

  // =============================================================================
  // PHASE 4: Carry Propagation and Result Assembly
  // =============================================================================
  // Extract carry bits from each product
  VU u_carry04 = ShiftRight<kHalfShift>(u_m04);
  VU u_carry02 = ShiftRight<kHalfShift>(u_m02);
  VU u_carry03 = ShiftRight<kHalfShift>(u_m03);
  VU u_carry01 = ShiftRight<kHalfShift>(u_m01);
  VU u_carry10 = ShiftRight<kHalfShift>(u_m10);

  // Extract lower halves
  VU u_low04 = And(u_m04, u_low_mask);
  VU u_low02 = And(u_m02, u_low_mask);
  VU u_low05 = And(u_m05, u_low_mask);
  VU u_low03 = And(u_m03, u_low_mask);

  // Column-wise accumulation (Intel SVML pattern)
  VU u_col3 = Add(u_low05, Add(u_m14, u_carry04));
  VU u_col2 = Add(u_low04, Add(u_m13, u_carry03));
  VU u_col1 = Add(u_low02, Add(u_m11, u_carry01));
  VU u_col0 = Add(u_low03, Add(u_m12, u_carry02));

  // Carry propagation through columns
  VU u_sum0 = Add(u_carry10, u_col1);
  VU u_carry_final0 = ShiftRight<kHalfShift>(u_sum0);
  VU u_sum1 = Add(u_carry_final0, u_col0);
  VU u_carry_final1 = ShiftRight<kHalfShift>(u_sum1);
  VU u_sum1_shifted = ShiftLeft<kHalfShift>(u_sum1);
  VU u_sum2 = Add(u_carry_final1, u_col2);
  VU u_carry_final2 = ShiftRight<kHalfShift>(u_sum2);
  VU u_sum3 = Add(u_carry_final2, u_col3);

  // Assemble final result
  VU u_result0 = And(u_sum0, u_low_mask);
  VU u_result2 = And(u_sum2, u_low_mask);
  VU u_result3 = ShiftLeft<kHalfShift>(u_sum3);

  VU u_reduce_lo = Add(u_sum1_shifted, u_result0);
  VU u_reduce_hi = Add(u_result3, u_result2);

  // =============================================================================
  // PHASE 5: Extract Quotient and Fractional Parts
  // =============================================================================

  // Extract integer quotient
  constexpr int kQuotientShift =
      ExponentBits<T>() + 1;  // 9 for F32, 12 for F64
  VU u_shifted_n = ShiftRight<kQuotientShift>(u_reduce_hi);

  // fractional shifts derived from magic constants
  // F32: 5, 18, 14 (sum = 37, total with quotient = 46 = 2×23)
  // F64: 28, 24, 40 (sum = 92, total with quotient = 104 = 2×52)
  constexpr int kFracLowShift = kIsSingle ? 5 : 28;
  constexpr int kFracMidShift = kIsSingle ? 18 : 24;
  constexpr int kFracHighShift = kIsSingle ? 14 : 40;

  // Verify total shift constraint
  constexpr int kTotalShift =
      kQuotientShift + kFracLowShift + kFracMidShift + kFracHighShift;
  static_assert(kTotalShift == (kIsSingle ? 46 : 104),
                "Total shift must equal 2×mantissa_bits");

  // Extract fractional parts
  constexpr TU kFracMidMask = (static_cast<TU>(1) << kFracMidShift) - 1;
  VU u_frac_low_bits = And(u_reduce_lo, Set(du, kFracMidMask));
  VU u_shifted_sig_lo = ShiftLeft<kFracLowShift>(u_frac_low_bits);
  VU u_frac_mid_bits = ShiftRight<kFracMidShift>(u_reduce_lo);

  // =============================================================================
  // PHASE 6: Magic Number Conversion to Floating Point
  // =============================================================================
  // magic constants for branchless int→float conversion
  // Handle sign bit
  VU u_sign_bit = And(BitCast(du, x), Set(du, SignMask<T>()));
  VU u_exponent_part =
      Xor(u_sign_bit, BitCast(du, Set(d, static_cast<T>(1.0))));
  VU u_quotient_signed = Or(u_shifted_n, u_exponent_part);

  // Magic number conversion for quotient
  V shifter = Set(d, kIsSingle ? 0x1.8p15f : 0x1.8p43);
  V integer_part = Add(shifter, BitCast(d, u_quotient_signed));

  V n = Sub(integer_part, shifter);
  V reduced_hi = Sub(BitCast(d, u_quotient_signed), n);

  // constants for fractional parts
  VU u_epsilon_low = BitCast(du, Set(d, kIsSingle ? 0x1p-46f : 0x1p-104));
  VU u_exp_low = Xor(u_sign_bit, u_epsilon_low);
  VU u_frac_low_combined = Or(u_shifted_sig_lo, u_exp_low);

  constexpr TU kFracHighMask =
      (static_cast<TU>(1) << (kFracHighShift - kFracLowShift)) - 1;
  VU u_frac_high_bits = And(u_reduce_hi, Set(du, kFracHighMask));
  V shifter_lo = BitCast(d, u_exp_low);
  V reduced_lo = Sub(BitCast(d, u_frac_low_combined), shifter_lo);

  VU u_epsilon = BitCast(du, Set(d, kIsSingle ? 0x1p-23f : 0x1p-52));
  VU u_exp_mid = Xor(u_sign_bit, u_epsilon);
  VU u_shifted_sig_mid =
      Or(ShiftLeft<kFracHighShift>(u_frac_high_bits), u_frac_mid_bits);
  VU u_frac_mid_combined = Or(u_shifted_sig_mid, u_exp_mid);
  V shifter_mid = BitCast(d, u_exp_mid);
  V reduced_med = Sub(BitCast(d, u_frac_mid_combined), shifter_mid);

  // =============================================================================
  // PHASE 7: Convert to Radians
  // =============================================================================

  // High-precision 2π constants
  V _2pu_lead = Set(d, kIsSingle ? 0x1.921fb6p2f : 0x1.921fb54442d18p+2);
  V _2pu_trail = Set(d, kIsSingle ? -0x1.777a5cp-23f : 0x1.1a62633145c07p-52);

  // Compensated summation
  V r_hi = Add(reduced_hi, reduced_med);
  V reduced_hu_err = Sub(reduced_hi, r_hi);
  V reduced_med_corr = Add(reduced_med, reduced_hu_err);
  V r_lo = Add(reduced_med_corr, reduced_lo);

  // Multiply by π with error compensation (Cody-Waite multiplication)
  V red_hi = Mul(r_hi, _2pu_lead);
  V mult_error = MulSub(r_hi, _2pu_lead, red_hi);
  V red_lo_final_part = MulAdd(_2pu_trail, r_hi, mult_error);
  V red_lo_final = MulAdd(_2pu_lead, r_lo, red_lo_final_part);

  // =============================================================================
  // PHASE 8: Small Argument Handling
  // =============================================================================

  const V min_input = Set(d, static_cast<T>(0x1p-20));
  const auto ismall_arg = Gt(min_input, abx);

  V r = IfThenElse(ismall_arg, x, red_hi);
  V e = IfThenElse(ismall_arg, Zero(d), red_lo_final);

  // Calculate table index
  VU u_n_mask = Set(du, kIsSingle ? 0xFF : 0x1FF);
  VU u_index = And(BitCast(du, integer_part), u_n_mask);
  VI u_table_index = BitCast(di, ShiftLeft<2>(u_index));

  // =============================================================================
  // PHASE 9: Table Lookup
  // =============================================================================

  const T *table_base = IS_COS ? kCosApproxTable<T> : kSinApproxTable<T>;

  // Gather table values: [deriv_error, sin_hi, sin_lo, deriv]
  // Based on Sollya script: p0=deriv_error, p1=sin_hi, p2=sin_lo,
  // p3=deriv
  V deriv_error = GatherIndex(d, table_base, u_table_index);
  V deriv = GatherIndex(d, table_base + 1, u_table_index);
  V sin_hi = GatherIndex(d, table_base + 2, u_table_index);
  V sin_lo = GatherIndex(d, table_base + 3, u_table_index);
  // =============================================================================
  // PHASE 10: Final Assembly
  // =============================================================================

  V r2 = Mul(r, r);
  // Apply first-order correction: sin_hi + deriv × r
  V first_order = MulAdd(deriv, r, sin_hi);
  V linear_error = MulAdd(deriv, r, Sub(sin_hi, first_order));

  // Apply derivative error correction
  V deriv_corrected = MulAdd(r, deriv_error, first_order);
  V deriv_correction_error =
      MulAdd(r, deriv_error, Sub(first_order, deriv_corrected));
  V total_linear_error = Add(deriv_correction_error, linear_error);

  // Polynomial corrections
  V s2 = Set(d, kIsSingle ? 0x1.1110b8p-7f : 0x1.1110fabb3551cp-7);
  V s1 = Set(d, kIsSingle ? -0x1.555556p-3f : -0x1.5555555554448p-3);
  V sin_poly = MulAdd(s2, r2, s1);
  sin_poly = Mul(sin_poly, r2);
  sin_poly = Mul(sin_poly, r);

  V c2 = Set(d, kIsSingle ? 0x1.5554f8p-5f : 0x1.5555555554ccfp-5);
  V c1 = Set(d, static_cast<T>(-0.5));
  V cos_poly;
  if constexpr (kIsSingle) {
    cos_poly = MulAdd(c2, r2, c1);
  } else {
    V c3 = Set(d, -0x1.6c16ab163b2d7p-10);
    cos_poly = MulAdd(c3, r2, c2);
    cos_poly = MulAdd(r2, cos_poly, c1);
  }
  cos_poly = Mul(cos_poly, r2);
  // Apply cross-term corrections
  V func_deriv_sum = Add(deriv_error, deriv);
  V corr1 = NegMulAdd(sin_hi, r, func_deriv_sum);
  // Apply remaining corrections
  V corr = MulAdd(corr1, e, sin_lo);
  V poly_correction = MulAdd(func_deriv_sum, sin_poly, total_linear_error);
  V cos_correction = MulAdd(sin_hi, cos_poly, corr);
  V final_correction = Add(cos_correction, poly_correction);
  return Add(deriv_corrected, final_correction);
}
// NOLINTNEXTLINE(google-readability-namespace-comments)
}  // namespace npsr::HWY_NAMESPACE::sincos
HWY_AFTER_NAMESPACE();

#endif  // NPSR_TRIG_LARGE_INL_H_
